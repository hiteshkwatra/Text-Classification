{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hites\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKING THE DICTIONARY OF VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeVocab():\n",
    "    dicionaryOfWords={}\n",
    "\n",
    "#making the list of stopwords\n",
    "\n",
    "    stop= open(r\"C:\\Users\\hites\\stopwords.txt\",\"r\")\n",
    "    StopWords= stop.read()\n",
    "    listOfStopWords= StopWords.split()\n",
    "    stop.close()\n",
    "    \n",
    "    \n",
    "#iterate over each folder ie each class that data has to be divided into\n",
    "\n",
    "    folders = [f for f in listdir(r\"C:\\Users\\hites\\20_newsgroups\")]\n",
    "    for eachfolder in folders:\n",
    "        print('reading folder',folders.index(eachfolder))\n",
    "        dicionaryOfWords=readEveryFile(eachfolder,dicionaryOfWords,listOfStopWords)\n",
    "        print(\"length of dic=\",len(dicionaryOfWords))\n",
    "        \n",
    "    return dicionaryOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def readEveryFile(foldername,dictionary,listStopWords):\n",
    "    folderloc=\"C://Users//hites//20_newsgroups//\" +foldername\n",
    "    \n",
    "    myfiles = [f for f in listdir(folderloc) if isfile(join(folderloc, f))]\n",
    "    \n",
    "    #all documents of current class\n",
    "    for eachfile in myfiles:\n",
    "        filename=eachfile\n",
    "        fileloc= folderloc+ \"//\"+filename\n",
    "        \n",
    "        file=open(fileloc,'r')\n",
    "        line= file.read()\n",
    "        \n",
    "        dictionary=makedictionary(line,dictionary,listStopWords)\n",
    "        file.close()\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makedictionary(line, dic,StopWords):\n",
    "    #extracting words using regular expressions\n",
    "    listofwords= re.findall(r\"[\\w']+\", line)\n",
    "    \n",
    "    #using lemmatizzer to autocrrect some spelling mistake in the words\n",
    "    for word in listofwords:\n",
    "        word= word.lower()\n",
    "        word=lmtzr.lemmatize(word)\n",
    "        \n",
    "        #not including words with length less than 2, because words with 2 length don't generally distinguish the documents\n",
    "        if len(word)<=2 :\n",
    "            continue\n",
    "            \n",
    "        #ignoring stopwords    \n",
    "        if word in StopWords:\n",
    "            continue\n",
    "            \n",
    "        #if word already in dictionary increaing its frequency\n",
    "        if word in dic :\n",
    "            dic[word]= dic[word]+1\n",
    "        else:\n",
    "            dic[word]=1\n",
    "            \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading folder 0\n",
      "length of dic= 18119\n",
      "reading folder 1\n",
      "length of dic= 32223\n",
      "reading folder 2\n",
      "length of dic= 84751\n",
      "reading folder 3\n"
     ]
    }
   ],
   "source": [
    "#function that returns dictionary of the distinct words in the whole document\n",
    "dictionary=makeVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=list(dictionary.values())\n",
    "Y=np.array(y)\n",
    "x=len(Y)\n",
    "X=np.arange(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## EDITING THE VOCAB. DICTIONARY BY REMOVING ENTERIES WITH FREQ. LESS THAN 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldic={}\n",
    "\n",
    "y=list(dictionary.values())\n",
    "a=0\n",
    "for i in dictionary.keys():\n",
    "    if y[a] >35:\n",
    "        finaldic[i]=y[a]   \n",
    "    a+=1\n",
    "Y=np.array(list(finaldic.values()))\n",
    "X=np.arange(len(Y))\n",
    "len(finaldic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#freqdic={}\n",
    "#for i in frequency:\n",
    "#    if i in freqdic:\n",
    "#        freqdic[i]+=1\n",
    "#    else:\n",
    "#        freqdic[i]=1\n",
    "#freqdic  \n",
    "#Y=np.array(list(freqdic.keys()))\n",
    "#X=np.array(list(freqdic.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot(X,Y,color=\"orange\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files again to create xtrain and ytrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maketrainingdata(dic):\n",
    "    \n",
    "    xtrain=[]\n",
    "    ytrain=[]\n",
    "    folders = [f for f in listdir(r\"C:\\Users\\hites\\20_newsgroups\")]\n",
    "    \n",
    "    for eachfolder in folders:\n",
    "        print('reading folder',folders.index(eachfolder))\n",
    "        smallXtrainList,  smallYtrainList= read(eachfolder,  finaldic,  folders.index(eachfolder))\n",
    "        \n",
    "        xtrain.extend(smallXtrainList)\n",
    "        ytrain.extend(smallYtrainList)\n",
    "        \n",
    "    return xtrain,ytrain\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read(foldername, dic,i):\n",
    "    #2d list for the data in each in category\n",
    "    eachcategoryXtrain=[]\n",
    "\n",
    "    folderloc=\"C://Users//hites//20_newsgroups//\" +foldername\n",
    "    \n",
    "    myfiles = [f for f in listdir(folderloc) if isfile(join(folderloc, f))]\n",
    "    \n",
    "    \n",
    " #1D list of category regarding each point  \n",
    "    eachcategoryYtrain= [i]* len(myfiles)\n",
    "    \n",
    "#reading every document\n",
    "    for eachfile in myfiles:\n",
    "        filename=eachfile\n",
    "        fileloc= folderloc+ \"//\"+filename\n",
    "        \n",
    "        file=open(fileloc,'r')\n",
    "        \n",
    "        line= file.read()\n",
    "        \n",
    "        \n",
    "        onepoint=trainone(line,dic)\n",
    "        \n",
    "        \n",
    "        eachcategoryXtrain.append(onepoint)\n",
    "        \n",
    "        file.close()\n",
    "    return eachcategoryXtrain,eachcategoryYtrain\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that returns single index value for the Xtrain\n",
    "\n",
    "def trainone(line, dic):\n",
    "    listofwords= re.findall(r\"[\\w']+\", line)\n",
    "    \n",
    "    #startin with each value of freq with 0\n",
    "    trainpoint=[0]* len(dic)\n",
    "    \n",
    "    values=list(dic.keys())\n",
    "    for word in listofwords :\n",
    "        if word in dic:\n",
    "            ind= values.index(word)\n",
    "            trainpoint[ind]+=1\n",
    "            \n",
    "    return trainpoint\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have Xtrain and ytrain\n",
    "XTRAINDATA, YTRAINDATA= maketrainingdata(finaldic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=np.array(XTRAINDATA)\n",
    "y_train=np.array(YTRAINDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrn,xtst,ytrn,ytst= train_test_split(x_train,y_train,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg=MultinomialNB()\n",
    "alg.fit(xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred=alg.predict(xtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytst,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Dictionary for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(xtrain,ytrain):\n",
    "    dic={}\n",
    "    classes= set(ytrain)\n",
    "    dic[\"total_training_count\"]=len(ytrain)\n",
    "    \n",
    "    for current_class in classes:\n",
    "        dic[current_class]={}\n",
    "        training_points_of_current_class= ytrain==current_class\n",
    "        current_xtrain=xtrain[training_points_of_current_class]\n",
    "        current_ytrain=ytrain[training_points_of_current_class]\n",
    "        \n",
    "        dic[current_class][\"total_current_class_points\"]=(training_points_of_current_class).sum()\n",
    "        \n",
    "        dic[current_class][\"total_count_of_words_current_class\"]=0\n",
    "        \n",
    "        for j in range(xtrain.shape[1]):\n",
    "            \n",
    "            #total number of times the word coming in each class\n",
    "            dic[current_class][j]= (current_xtrain[:,j]).sum()\n",
    "            #total number of words coming in current class\n",
    "            dic[current_class][\"total_count_of_words_current_class\"]=dic[current_class][\"total_count_of_words_current_class\"]+(current_xtrain[:,j]).sum()\n",
    "            \n",
    "    return dic\n",
    "            \n",
    "            \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(dic,curr_class,x):\n",
    "    output= np.log(dic[curr_class][\"total_current_class_points\"]) - np.log(dic[\"total_training_count\"])\n",
    "\n",
    "    number_of_features= len(dic[curr_class].keys()) -2\n",
    "    \n",
    "    for j in range(number_of_features):\n",
    "        xj= x[j]\n",
    "        \n",
    "        #xj==0 means word does come in the dictionary\n",
    "        if xj==0:\n",
    "            continue\n",
    "        count_of_current_word= dic[curr_class][j] +1\n",
    "        \n",
    "        count_of_total_words_in_current_class= dic[curr_class][\"total_count_of_words_current_class\"] + len(dic[curr_class].keys()) -2\n",
    "        current_word_pro= np.log(count_of_current_word)- np.log(count_of_total_words_in_current_class)\n",
    "        output= output + current_word_pro\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_one(dic,xtest):\n",
    "    class_prob=-1000\n",
    "    class_pred=-1\n",
    "    A= True\n",
    "    for current_class in dic.keys():\n",
    "        if current_class== \"total_training_count\":\n",
    "            continue\n",
    "        value=probability(dic,current_class,xtest)\n",
    "      \n",
    "        \n",
    "        \n",
    "        if A or value > class_prob:\n",
    "            class_prob=value\n",
    "            class_pred=current_class\n",
    "        A=False\n",
    "    \n",
    "    return class_pred\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(dic,xtest):\n",
    "    ypred=[]\n",
    "    i=0\n",
    "    for x in xtest:\n",
    "        i=i+1\n",
    "        print(i, end=\" \")\n",
    "        y=predict_one(dic,x)\n",
    "        ypred.append(y)\n",
    "    return ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary=fit(xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtst[1,:].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredicted=predict(dictionary,xtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytst,ypredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
